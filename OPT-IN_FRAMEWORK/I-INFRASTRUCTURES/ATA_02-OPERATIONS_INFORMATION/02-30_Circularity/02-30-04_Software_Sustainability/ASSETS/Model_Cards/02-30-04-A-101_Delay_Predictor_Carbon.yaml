# Model Card: Flight Delay Predictor Neural Network
# With Carbon Footprint Tracking
# AMPEL360-02-30-04-A-101
# Version: 1.0.0
# Date: 2025-11-21

model_metadata:
  name: "Flight Delay Predictor NN"
  version: "2.1.0"
  model_id: "AMPEL-NN-DELAY-PRED-v2.1"
  model_type: "Neural Network (Deep Learning)"
  framework: "TensorFlow 2.14"
  created_date: "2024-10-15"
  last_trained: "2025-11-01"
  status: "Production"
  
model_description:
  purpose: "Predict flight delays based on weather, air traffic, aircraft performance, and operational factors"
  use_case: "Pre-flight planning and real-time operational decision support"
  target_variable: "Delay duration (minutes)"
  prediction_horizon: "Up to 24 hours before departure"
  
model_architecture:
  layers:
    - type: "Input"
      shape: [128]
      description: "Weather, traffic, aircraft, and operational features"
    - type: "Dense"
      units: 256
      activation: "ReLU"
    - type: "Dropout"
      rate: 0.2
    - type: "Dense"
      units: 128
      activation: "ReLU"
    - type: "Dropout"
      rate: 0.2
    - type: "Dense"
      units: 64
      activation: "ReLU"
    - type: "Dense"
      units: 1
      activation: "Linear"
      description: "Predicted delay (minutes)"
  
  parameters:
    total_params: 85_472
    trainable_params: 85_472
    non_trainable_params: 0
    
performance_metrics:
  test_set:
    mae: 8.3  # Mean Absolute Error (minutes)
    rmse: 12.7  # Root Mean Squared Error (minutes)
    r2_score: 0.82
    accuracy_within_15min: 0.89
    
  validation_set:
    mae: 8.1
    rmse: 12.4
    r2_score: 0.83
    
  production_monitoring:
    mae_rolling_30d: 8.9
    drift_score: 0.03  # Low drift
    
training_data:
  dataset_name: "AMPEL360 Historical Flight Operations"
  dataset_version: "v3.2"
  samples_total: 450_000
  samples_train: 360_000
  samples_validation: 45_000
  samples_test: 45_000
  time_period: "2022-01-01 to 2024-09-30"
  features:
    count: 128
    categories:
      - "Weather (temperature, wind, visibility, precipitation)"
      - "Air traffic (congestion, slot availability)"
      - "Aircraft (type, age, maintenance status)"
      - "Operational (route, time of day, season)"
  
# ============================================================================
# CARBON FOOTPRINT TRACKING
# ============================================================================

carbon_footprint:
  
  # Training Phase Carbon Footprint
  training:
    hardware:
      gpu_type: "NVIDIA A100 (80GB)"
      gpu_count: 4
      gpu_tdp_watts: 400
      cpu_type: "AMD EPYC 7763"
      cpu_count: 2
      cpu_tdp_watts: 280
      
    training_time:
      total_hours: 36.5
      epochs: 150
      batch_size: 512
      early_stopping_epoch: 142
      
    energy_consumption:
      gpu_energy_kwh: 58.4  # 4 GPUs × 400W × 36.5h
      cpu_energy_kwh: 20.4  # 2 CPUs × 280W × 36.5h
      other_energy_kwh: 8.2  # Memory, storage, cooling overhead
      total_energy_kwh: 87.0
      
    data_center:
      location: "Frankfurt, Germany"
      pue: 1.25  # Power Usage Effectiveness
      renewable_energy_pct: 85
      grid_emission_factor_kg_co2e_per_kwh: 0.420  # Germany grid average
      renewable_emission_factor_kg_co2e_per_kwh: 0.010  # Wind/solar with lifecycle
      
    carbon_emissions:
      # Scope 2 (electricity consumption)
      renewable_portion_kg_co2e: 0.74  # 87.0 kWh × 85% × 0.010
      grid_portion_kg_co2e: 5.48  # 87.0 kWh × 15% × 0.420
      total_scope2_kg_co2e: 6.22
      
      # Scope 3 (hardware embodied carbon allocated)
      hardware_embodied_kg_co2e: 0.45  # Amortized over hardware lifetime and utilization
      
      # Total training carbon footprint
      total_training_kg_co2e: 6.67
      total_training_t_co2e: 0.0067
      
    optimizations_applied:
      - "Mixed precision training (FP16) reducing compute by ~40%"
      - "Early stopping to prevent overtraining"
      - "Data center with high renewable energy mix"
      - "GPU utilization monitoring to maximize efficiency"
      
  # Inference Phase Carbon Footprint
  inference:
    deployment:
      infrastructure: "AWS EC2 g4dn.xlarge (NVIDIA T4)"
      gpu_tdp_watts: 70
      region: "eu-central-1 (Frankfurt)"
      pue: 1.12  # AWS data center
      renewable_energy_pct: 100  # AWS 100% renewable commitment
      
    performance:
      inference_time_ms: 15
      throughput_predictions_per_second: 65
      daily_predictions: 25_000  # Average production load
      
    energy_per_prediction:
      gpu_energy_wh: 0.00029  # 70W × (15ms / 3600000ms)
      other_energy_wh: 0.00012  # CPU, memory, network
      total_energy_wh: 0.00041
      total_energy_kwh: 0.00000041
      
    daily_energy:
      predictions: 25_000
      energy_kwh: 0.0103  # 25,000 × 0.00041 Wh / 1000
      
    carbon_emissions:
      # With 100% renewable energy (AWS commitment)
      emission_factor_kg_co2e_per_kwh: 0.010  # Lifecycle emissions
      daily_kg_co2e: 0.000103  # 0.0103 kWh × 0.010
      annual_kg_co2e: 0.038  # 365 days
      
      # Per prediction carbon intensity
      per_prediction_g_co2e: 0.0000041
      
  # Amortization of Training Carbon
  amortization:
    model_lifetime_months: 12  # Retrained annually
    total_predictions_lifetime: 9_125_000  # 25,000/day × 365 days
    training_carbon_per_prediction_g_co2e: 0.00073  # 6.67 kg / 9.125M predictions
    
  # Total Carbon Footprint per Prediction
  total_per_prediction:
    training_amortized_g_co2e: 0.00073
    inference_g_co2e: 0.0000041
    total_g_co2e: 0.00073  # Dominated by amortized training
    
  # Lifecycle Carbon Footprint
  lifecycle_total:
    training_kg_co2e: 6.67
    inference_annual_kg_co2e: 0.038
    hardware_embodied_allocated_kg_co2e: 0.45
    total_annual_kg_co2e: 7.16
    
  # Carbon Benefit Analysis
  carbon_benefit:
    description: "Delay prediction enables fuel-efficient flight planning and reduced reactive delays"
    estimated_fuel_savings_kg_per_avoided_delay: 250  # Average
    estimated_co2_savings_kg_per_avoided_delay: 788  # 250 kg fuel × 3.15 kg CO2/kg fuel
    delays_avoided_annually: 450  # Based on operational analysis
    annual_co2_savings_kg: 354_600  # 788 × 450
    
    net_carbon_benefit:
      annual_savings_kg_co2e: 354_600
      annual_footprint_kg_co2e: 7.16
      net_benefit_kg_co2e: 354_593
      roi_carbon: 49_528  # Savings / Footprint (nearly 50,000x return!)

# ============================================================================
# CONTINUOUS IMPROVEMENT
# ============================================================================

optimization_roadmap:
  completed:
    - "Mixed precision training (FP16)"
    - "Early stopping implementation"
    - "Green data center selection"
    
  in_progress:
    - "Model quantization for inference (INT8) - target 50% energy reduction"
    - "Federated learning exploration to reduce centralized training"
    
  planned:
    - "Knowledge distillation to smaller model (target 70% parameter reduction)"
    - "Carbon-aware training scheduling (run during high renewable availability)"
    - "Edge deployment for selected use cases"

# ============================================================================
# DOCUMENT CONTROL
# ============================================================================

document_control:
  generated_by: "AI (GitHub Copilot), prompted by Amedeo Pelliccia"
  status: "DRAFT"
  human_approver: "[to be completed]"
  repository: "AMPEL360-BWB-H2-Hy-E"
  last_ai_update: "2025-11-21"
  
compliance:
  - "EU AI Act (Environmental sustainability requirements)"
  - "AMPEL360 Green Software Principles"
  - "GHG Protocol for carbon accounting"
