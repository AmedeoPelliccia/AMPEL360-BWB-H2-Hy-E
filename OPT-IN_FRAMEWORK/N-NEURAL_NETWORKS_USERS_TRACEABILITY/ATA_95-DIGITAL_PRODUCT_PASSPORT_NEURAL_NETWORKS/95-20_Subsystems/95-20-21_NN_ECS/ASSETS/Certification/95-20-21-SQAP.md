
# **95-20-21 — Software Quality Assurance Plan (SQAP)**

### ECS Neural Network Subsystem (NN_ECS)

**Document ID**: 95-20-21-SQAP
**Applies To**: Models A-101 → A-107
**Certification Target**: **DO-178C Level C**
**Version**: 1.0
**Status**: WORKING
**Last Updated**: 2025-11-19

---

# **1. Purpose**

The Software Quality Assurance Plan (SQAP) describes the **quality principles, audits, reviews, controls, monitoring activities, and acceptance criteria** ensuring that the **ECS Neural Network Subsystem (95-20-21 NN_ECS)** satisfies:

* **DO-178C** quality assurance objectives for DAL C
* **EASA AI Roadmap 2.0 / SC-AI** requirements for AI-based systems
* **FAA AI Assurance** guidelines
* **ATA 95 Neural Network Digital Product Passport (DPP)** governance
* **AMPEL360 OPT-IN documentation & traceability rules**

This plan governs activities for the lifecycle of:

* Source code
* Neural network models (ONNX)
* Training datasets
* Synthetic data
* Documentation artefacts
* Certification evidence
* Tests and verification reports
* Supporting CI tools
* Traceability records

---

# **2. Software Covered by this SQAP**

This SQAP applies to all NN ECS components:

| Model ID | Function                    |
| -------- | --------------------------- |
| A-101    | Cabin Temperature Predictor |
| A-102    | Air Quality Monitor         |
| A-103    | HVAC Optimizer              |
| A-104    | Pressure Control NN         |
| A-105    | Humidity Manager            |
| A-107    | CO₂ Scrubbing Optimizer     |

Their full descriptions are in:
`ASSETS/Model_Cards/95-20-21-A-XXX_*.yaml`

---

# **3. Quality Assurance Objectives (DO-178C DAL C)**

The following objectives **must be satisfied**:

### **3.1 Process Assurance Objectives**

* Plans exist, are complete, and are followed.
* Software activities comply with approved standards.
* Proof that verification meets DO-178C Level C.
* Proper implementation of configuration management.
* Problem reporting system active and traceable.
* Evidence that tools used are controlled and assessed.

### **3.2 Product Assurance Objectives**

* Software requirements are correct, unambiguous, and testable.
* Software design is consistent with requirements.
* Code conforms to standards and design.
* Verification demonstrates correctness of implementation.
* All changes controlled and verified.
* Delivered product passes all audits (FCA + PCA).

---

# **4. Applicable Standards**

### **4.1 Primary Standards**

* DO-178C — Software Considerations in Airborne Systems
* DO-330 — Tool Qualification
* DO-331 — Model-Based Development
* DO-333 — Formal Methods
* DO-200B — Data Quality Requirements (for training datasets)

### **4.2 AI-specific Standards**

* **EASA SC-AI**
* **FAA AI Assurance**
* **EU AI Act — High-Risk Systems**
* **ATA 95 Digital Product Passport (NN governance)**

### **4.3 AMPEL360 Internal Standards**

* OPT-IN documentation rules
* ATA 95 folder skeleton
* Hyperlink-injected documentation
* doc-meta-enforcer
* Traceability enforcer
* ONNX integrity pipeline

---

# **5. Software Quality Assurance Activities**

QA activities ensure **objective, independent assessment** of:

* Requirements
* Design
* Source code
* Neural network models
* Datasets
* Tools
* Documentation
* Test results
* Configuration baselines

---

## **5.1 Reviews & Audits**

QA performs *independent* reviews of:

### **5.1.1 Requirements Review**

* Consistency & completeness across:
  `SRS → Model Card → Traceability CSV → Tests`
* All I/O match ATA-21 ECS signal specs.
* Safety constraints enforced (bounds, rates, units).

### **5.1.2 Design Review**

* Validation of the deterministic inference pipeline.
* NN wrapper logic correctness (pre/post-processing).
* Correct enforcement of safety envelopes.
* Integration with IMA partition APIs.

### **5.1.3 Code Review**

* Conformance to SCS (95-20-21-SCS).
* Static analysis:

  * No dynamic execution
  * No GPU-dependence in deployed runtime
  * No external side-effects
* Deterministic behaviour required.

### **5.1.4 Model Review**

For each ONNX model:

* Hash mismatch detection
* ONNX structure validity
* Input/Output tensor shape consistency
* Failure mode testing
* Drift detection & training data coverage

### **5.1.5 Dataset Quality Review**

Performed per **DO-200B**:

* Data integrity
* Schema compliance
* Label correctness
* Missing/invalid values
* Representativeness summary

Synthetic datasets reviewed for:

* Physical plausibility
* Distribution match to real flight data

### **5.1.6 Documentation Review**

Ensures:

* Compliance with OPT-IN metadata requirements
* Hyperlink completeness
* Cross-chapter referencing
* AI-assistance disclaimers
* Consistency with ATA95-00-00 GENERAL pattern

---

## **5.2 Verification Oversight**

QA verifies that:

* All tests defined in SVP are executed.
* Actual results match expected results.
* All anomalies recorded and closed.
* Coverage of wrapper code meets MC/DC DAL C objectives.
* No failing test is waived without justification.

---

## **5.3 Traceability Assessment**

QA ensures complete traceability across:

```
Requirement → Model Card → Source Code → ONNX Model → Test Case → Evidence Log
```

Mechanisms:

* TRACE CSV
* DPP entries
* CI hyperlink injection
* Static doc-meta audit

---

## **5.4 CI / CD Quality Controls**

The following automated quality gates are mandatory:

### **5.4.1 doc-meta-enforcer**

Checks:

* Document metadata
* Document control blocks
* Hyperlinks resolved
* AI-assistance acknowledgment

### **5.4.2 ONNX Integrity Pipeline**

Checks:

* Structural validity
* Operator sanity
* Unsupported ops
* Tensor shapes
* Hash consistency

### **5.4.3 Certification Compliance Checker**

Evaluates:

* Presence of all DO-178C artefacts
* Required certification files
* DPP requirements

### **5.4.4 Traceability Enforcer**

Checks:

* Broken traceability
* Missing test links
* Missing requirement coverage

---

# **6. Nonconformance & Problem Reporting**

All anomalies are documented as **Problem Reports**:

* Stored as:
  `Certification/PR_95-20-21-XXX.md`
* Each PR must include:

  * Description
  * Reproduction steps
  * Impact analysis
  * Affected CIs
  * DAL impact
  * Root cause
  * Resolution steps
  * Regression evidence
  * Approvals

PRs are closed only after:

* Verification
* QA sign-off
* CCB approval (if required)
* Complete documentation update

---

# **7. Corrective Actions**

Corrective actions follow a formal workflow:

1. Identify issue (PR opened).
2. Classify severity & DAL impact.
3. Define corrective plan.
4. Implement fix.
5. Perform re-verification.
6. Update documentation.
7. QA review.
8. CCB approval (if major).

---

# **8. Tools Quality Assurance**

### **8.1 Tools Under QA Oversight**

| Tool                        | QA Validation               | Rationale        |
| --------------------------- | --------------------------- | ---------------- |
| ONNX Runtime                | Inference consistency tests | Operational tool |
| tf2onnx / torch.onnx.export | Conversion verification     | Translation tool |
| Synthetic Data Generator    | Statistical validation      | Dataset CI       |
| doc-meta-enforcer           | Documentation integrity     | Artefact QA      |
| hyperlink injector          | DID compliance              | Traceability     |
| test_onnx_model.py          | Verification tool           | Non-qualified    |

### **8.2 Tool Qualification (DO-330)**

Per PSAC, **no tool requires formal DO-330 qualification**, but some require **“validation by use”**:

* ONNX Runtime
* Custom ECS NN wrapper library

Validation includes:

* Regression testing
* BIT & health monitoring
* Cross-platform consistency

---

# **9. Quality Records**

The following records are maintained under:

```
Certification/QA_Records/
```

Includes:

* Review logs
* QA checklists
* Inspection reports
* FCA / PCA audit logs
* Tool validation reports
* Dataset integrity logs
* Drift detection summaries

Retention period: **≥ 10 years after aircraft retirement**

---

# **10. Deliverables & Acceptance Criteria**

A CI is **acceptable** when:

* All QA reviews passed
* Traceability is complete
* Tests passed with evidence stored
* Model Card validated
* ONNX hash verified
* DPP entry updated
* SCMP-controlled version assigned
* No open PRs for that CI

Only then may it enter the **Certification Baseline**.

---

# **11. Document Control**

* **Document ID**: 95-20-21-SQAP
* **Version**: 1.0
* **Status**: WORKING
* **Author**: AMPEL360 ML Engineering Team
* **AI Assistance**: GitHub Copilot + ChatGPT
* **Prompted by**: *Amedeo Pelliccia*
* **Review Required**: YES, Certification Authority

---


