
# **95-20-21 — Software Verification Plan (SVP)**

### ECS Neural Network Subsystem (NN_ECS)

**Document ID**: 95-20-21-SVP
**Applies To**: Models A-101 → A-107
**Certification Target**: **DO-178C Level C**
**Version**: 1.0
**Status**: WORKING
**Last Updated**: 2025-11-19

---

# **1. Purpose**

This Software Verification Plan (SVP) defines the **verification strategy, methods, environments, test procedures, coverage requirements, and evidence** to demonstrate compliance of the **ECS Neural Network subsystem (95-20-21 NN_ECS)** with:

* **RTCA DO-178C DAL C**
* **DO-330 Tool Qualification**
* **DO-331 Model-Based Development**
* **DO-333 Formal Methods** (NN safety shell reasoning)
* **EASA SC-AI**
* **FAA AI Assurance Framework**
* **ATA 95 DPP governance**
* **AMPEL360 OPT-IN Documentation Standards**

Verification covers:

* Requirements
* Interfaces
* Design
* Source Code
* Neural Network models
* Datasets
* Safety envelopes
* Fallback logic
* Runtime behaviour
* Timing requirements
* Integration with ATA-21

---

# **2. Software Covered by this SVP**

Applies to the following ECS NN components:

| Model ID | Function                    |
| -------- | --------------------------- |
| A-101    | Cabin Temperature Predictor |
| A-102    | Air Quality Monitor         |
| A-103    | HVAC Optimizer              |
| A-104    | Pressure Control NN         |
| A-105    | Humidity Manager            |
| A-107    | CO₂ Scrubbing Optimizer     |

All model descriptors are in:
`ASSETS/Model_Cards/95-20-21-A-XXX_*.yaml`

All ONNX artefacts reside in:
`Models/Trained/`

---

# **3. Verification Approach**

Verification is **requirements-based**, aligned with DO-178C DAL C objectives, extended for neural networks using:

* Statistical verification
* Robustness testing
* Out-of-distribution testing
* Scenario-based testing (Digital Twin)
* Deterministic wrapper testing
* NN vs ECS classical controller consistency testing

Verification follows three independent axes:

1. **Functional Verification**
2. **Robustness & Safety Verification**
3. **Integration Verification**

---

# **4. Verification Levels**

### **4.1 High-Level Requirements (HLR) Verification**

* Derived from SRS (95-20-21-SRS).
* Demonstrated through:

  * Functional tests
  * Data-driven scenario tests
  * Consistency checks with ATA-21

### **4.2 Low-Level Requirements (LLR) Verification**

Covers:

* NN wrapper logic
* Input validation
* Output clipping
* Confidence monitoring
* Fallback triggers
* Exceptions and error handling

### **4.3 Software Integration Verification**

Executed within the **IMA partition (95-20-42)**.

Validates:

* Timing (10 Hz)
* Memory budget
* Inter-process communication
* Deterministic execution
* Fallback activation
* Health monitoring

### **4.4 Hardware/Software Integration**

Ensures:

* Correct deployment
* ONNX engine behaviour matches expectations
* No unsupported operators

---

# **5. Verification Methods**

| Method                        | Purpose                                       |
| ----------------------------- | --------------------------------------------- |
| **Analysis**                  | Requirements, design, envelopes, safety logic |
| **Review**                    | Architecture, code, datasets, ONNX structure  |
| **Test**                      | Functional, integration, stress, timing       |
| **Formal Reasoning (DO-333)** | Safety-shell correctness                      |
| **Simulation**                | Digital Twin ECS scenario validation          |
| **Statistical Testing**       | NN prediction confidence & robustness         |

---

# **6. Verification Environments**

### **6.1 Development Test Environment**

* Python 3.10
* TensorFlow / PyTorch
* ONNX Runtime
* pytest, coverage.py
* Dockerized CI

### **6.2 Integration Environment**

* IMA virtualized environment (ARINC-653)
* ONNX Runtime compiled for target
* Real-time scheduler simulation

### **6.3 Digital Twin Environment**

Used for scenario testing:
`Data/Synthetic_Data/95-20-21-605_Digital_Twin_ECS_Scenarios.parquet`

Includes:

* Temperature transients
* Humidity spikes
* Scrubber degradation
* Air quality events
* Extreme weather
* Passenger load shocks

### **6.4 Hardware-in-the-Loop (HIL)**

Late-stage verification using:

* Real ECS controller logic
* Simulated sensor streams
* Full timing fidelity

---

# **7. Test Cases**

### **7.1 Functional Tests**

Validate correctness per requirement.

#### Examples:

* Temperature predictor accuracy < ±0.5°C
* AQI classification accuracy > 98%
* HVAC optimizer must propose bounded, rate-limited setpoints
* Pressure control NN must not violate cabin pressurization curve

Stored under:
`Tests/Functional/`

---

### **7.2 Robustness Tests**

#### Tests include:

* Noisy sensor data
* Out-of-range values
* Missing data / dropout
* Delay & jitter injection
* Sudden ECS state changes
* Adversarial noise injection
* Extreme conditions (< -30°C or > +40°C)

Stored under:
`Tests/Robustness/`

---

### **7.3 Safety Tests**

Verify:

* Compliance with safety envelopes
* Correct fallback activation when:

  * Confidence < threshold
  * Sensor invalid
  * NN runtime error
  * Input out-of-distribution
* No unsafe actuator request even if NN fails

Stored under:
`Tests/Safety/`

---

### **7.4 Integration Tests**

Validating:

* Communication with ATA-21 classical controller
* Setpoint validation layer
* Safety shell behaviour
* Combined multi-NN integration (A-101 + A-102 + A-105 + A-107 feeding A-103)
* IMA partition latency and jitter

Stored under:
`Tests/Integration/`

---

### **7.5 Performance & Timing Tests**

Evaluate:

* Inference time < 10 ms
* Stable runtime < 50 MB RAM
* CPU < 0.5 core average
* Deterministic outputs in repeated runs

Stored under:
`Tests/Performance/`

---

### **7.6 ONNX Structural Tests**

Performed using:
`Models/Scripts/inspect_onnx_model.py`

Includes checks for:

* Unsupported ops
* Invalid shapes
* Dead nodes
* Floating-point anomalies
* Missing metadata

Stored under:
`Tests/ONNX/`

---

### **7.7 Dataset Verification Tests**

Per DO-200B:

* Schema compliance
* Statistical distribution checks
* Representativeness assessment
* Missing-data analysis
* Cross-aircraft consistency

Stored under:
`Tests/Data/`

---

# **8. Coverage Analysis**

### **8.1 Code Coverage**

Measured on deterministic wrapper code only:

* Input validators
* Output clipper
* Fallback handlers
* Error handling
* State machine (if applicable)

Tools:

* coverage.py
* pytest-cov

Target:

* **MC/DC coverage for all safety-relevant logic**

### **8.2 NN Model Coverage**

As permitted by SC-AI:

* Internal NN graph is **not MC/DC covered**
* Instead:

  * Statistical coverage
  * Scenario coverage
  * Boundary coverage
  * Latent hazard coverage

Metrics:

* Confidence distribution
* Drift curves
* OOD detection rate

---

# **9. Verification Traceability**

Traceability is enforced through:

### Files:

* `TRACE/95-20-21-TRACE.csv`
* `Model Cards`
* `Tests/*`
* `Certification/Verification_Reports/`

### Links:

```
Requirement → Test Case → Test Result → Evidence → ONNX Hash → DPP Entry
```

---

# **10. Verification Deliverables**

### The following verification artefacts will be produced:

| Deliverable               | Location                    |
| ------------------------- | --------------------------- |
| Test Procedures           | Tests/                      |
| Test Results              | ASSETS/Reports/             |
| Coverage Reports          | ASSETS/Reports/Coverage/    |
| ONNX Inspection Logs      | ASSETS/Reports/ONNX/        |
| Dataset QC Reports        | Data/Reports/               |
| Timing & Performance Logs | ASSETS/Reports/Performance/ |
| FCA/PCA Logs              | Certification/Audits/       |

---

# **11. Non-Conformance Handling**

All test failures create:

* A Problem Report (`PR_95-20-21_XXX.md`)
* Linked issues in GitHub
* Automatic CI annotation

Resolution requires:

* Fix
* Regression test
* QA approval
* CCB approval (if major)

---

# **12. Acceptance Criteria**

A model/software component is “verified” when:

* All tests passed
* Coverage targets met
* Traceability complete
* ONNX hash validated
* DPP entry updated
* No open PRs
* QA review complete

Only then it is eligible for entry into the **Certification Baseline**.

---

# **13. Document Control**

* **Document ID**: 95-20-21-SVP
* **Version**: 1.0
* **Status**: WORKING
* **Author**: AMPEL360 ML Engineering Team
* **AI Assistance**: ChatGPT + GitHub Copilot
* **Prompted by**: *Amedeo Pelliccia*
* **Review Required**: Certification Authority

---
