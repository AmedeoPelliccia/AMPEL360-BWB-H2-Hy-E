#!/usr/bin/env python3
"""
doc_meta_enforcer.py

Automates:
- Adding AI generation note into Document Control sections
- Adding CAOS (Computer Aided Operations and Services) awareness
- Linking internal document references
- Generating stub documents if referenced but missing

Covers all OPT-IN_FRAMEWORK areas:
- I-INFRASTRUCTURES (Infrastructure and Foundational Systems)
- N-NEURAL_NETWORKS_USERS_TRACEABILITY (Neural Networks and AI)
- O-ORGANIZATION (Organizational Structure and Governance)
- P-PROGRAM (Program Management and Planning)
- T-TECHNOLOGY_AMEDEOPELLICCIA-ON_BOARD_SYSTEMS (Technology and On-Board Systems)

Usage:
  # Check only (CI) - ATA_02 only
  python tools/ci/doc_meta_enforcer.py --check

  # Auto-fix locally - ATA_02 only
  python tools/ci/doc_meta_enforcer.py --fix

  # Retrofit entire OPT-IN_FRAMEWORK (all 5 areas, ~15K files)
  python tools/ci/doc_meta_enforcer.py --fix --scope entire

  # Check entire repository with CAOS awareness
  python tools/ci/doc_meta_enforcer.py --check --scope full
"""

import argparse
import json
import pathlib
import re
import sys
from typing import List, Tuple

# This script lives in repo_root/tools/ci/
REPO_ROOT = pathlib.Path(__file__).resolve().parents[2]

# Default to ATA_02 for backward compatibility, but can scan entire framework
DEFAULT_DOC_ROOT = (
    REPO_ROOT
    / "OPT-IN_FRAMEWORK"
    / "I-INFRASTRUCTURES"
    / "ATA_02-OPERATIONS_INFORMATION"
)

ENTIRE_FRAMEWORK_ROOT = REPO_ROOT / "OPT-IN_FRAMEWORK"

AI_LINE = (
    "- Generated by: AI (prompted by Amedeo Pelliccia); "
    "pending approval by [Approver]"
)

# CAOS (Computer Aided Operations and Services) awareness
CAOS_LINE = (
    "- CAOS Integration: This document is part of the Computer Aided Operations and Services framework"
)

# We treat these as "internal" docs that can be stubbed & linked
INTERNAL_EXTS = {".md", ".csv", ".json"}

# OPT-IN Framework structure
FRAMEWORK_AREAS = {
    "I-INFRASTRUCTURES": "Infrastructure and Foundational Systems",
    "N-NEURAL_NETWORKS_USERS_TRACEABILITY": "Neural Networks and AI Systems",
    "O-ORGANIZATION": "Organizational Structure and Governance",
    "P-PROGRAM": "Program Management and Planning",
    "T-TECHNOLOGY_AMEDEOPELLICCIA-ON_BOARD_SYSTEMS": "Technology and On-Board Systems"
}


def find_component_root(path: pathlib.Path, doc_root: pathlib.Path) -> pathlib.Path:
    """
    Given a file like
    .../02-11-00_AIRCRAFT_DIMENSIONS_GEOMETRY/07_V_AND_V/...
    return the component root
    .../02-11-00_AIRCRAFT_DIMENSIONS_GEOMETRY
    
    Also handles other OPT-IN_FRAMEWORK structures like:
    .../ATA_XX-COMPONENT_NAME/...
    """
    p = path
    while p != REPO_ROOT:
        # Match ATA chapter pattern: 02-11-00_NAME or ATA_XX-NAME
        if re.match(r"(\d{2}-\d{2}-\d{2}_.+|ATA_\d{2}-.+)", p.name):
            return p
        p = p.parent
    return doc_root


def is_caos_related(text: str, path: pathlib.Path) -> bool:
    """
    Determine if document is CAOS (Computer Aided Operations and Services) related.
    """
    # Check if CAOS is mentioned in the document
    if "CAOS" in text.upper() or "Computer Aided Operations" in text:
        return True
    
    # Check if path contains CAOS-related keywords
    path_str = str(path).upper()
    caos_keywords = ["OPERATIONS", "NEURAL", "AI", "AUTOMATION", "SERVICES"]
    return any(keyword in path_str for keyword in caos_keywords)


def ensure_ai_line_in_doc_control(text: str, path: pathlib.Path = None, add_caos: bool = False) -> Tuple[str, bool]:
    """
    Ensure Document Control section contains AI_LINE and optionally CAOS_LINE.
    Returns (new_text, changed?).
    """
    lines = text.splitlines()
    changed = False

    for i, line in enumerate(lines):
        if "Document Control" in line:
            # Scan following lines until blank or next header
            j = i + 1
            has_ai = False
            has_caos = False
            
            while j < len(lines) and lines[j].strip() and not lines[j].startswith("#"):
                if "Generated by:" in lines[j] or "Generated by AI" in lines[j]:
                    has_ai = True
                if "CAOS Integration" in lines[j]:
                    has_caos = True
                j += 1
            
            insert_pos = i + 1
            
            # Add AI line if missing
            if not has_ai:
                lines.insert(insert_pos, AI_LINE)
                changed = True
                insert_pos += 1
            
            # Add CAOS line if appropriate and missing
            if add_caos and not has_caos and path and is_caos_related(text, path):
                lines.insert(insert_pos, CAOS_LINE)
                changed = True
            
            break  # assume only one Document Control section

    return "\n".join(lines), changed


def find_internal_paths(text: str) -> List[str]:
    """
    Find internal file-like references such as:
      01_OVERVIEW/baseline_dimensions.json
      07_V_AND_V/DIMENSION_VERIFICATION/VER-02-11-001_Wingspan_Measurement.md

    Very simple heuristic: something with slashes + known extension.
    """
    pattern = r"(?P<path>\b[0-9]{2}_[A-Z0-9][A-Za-z0-9_/-]*/[^\s`\)]+?\.(?:md|csv|json))"
    return sorted(set(m.group("path") for m in re.finditer(pattern, text)))


def link_internal_paths(
    text: str, component_root: pathlib.Path
) -> Tuple[str, List[pathlib.Path], bool]:
    """
    Turn bare paths into markdown links if not already linked.
    Also collect paths that don't exist for possible stub generation.
    Returns (new_text, missing_paths, changed?).
    """
    missing: List[pathlib.Path] = []
    changed = False

    def repl(match: re.Match) -> str:
        nonlocal changed, missing
        path_str = match.group("path")

        # If already part of a markdown link, don't touch
        # e.g. [something](01_OVERVIEW/xxx.md)
        pre = text[max(0, match.start() - 2) : match.start()]
        if pre == "](":
            return path_str

        target = component_root / path_str
        if not target.exists():
            missing.append(target)

        changed = True
        return f"[{path_str}]({path_str})"

    pattern = r"(?P<path>\b[0-9]{2}_[A-Z0-9][A-Za-z0-9_/-]*/[^\s`\)]+?\.(?:md|csv|json))"
    new_text = re.sub(pattern, repl, text)
    return new_text, missing, changed


def create_stub(path: pathlib.Path) -> None:
    """
    Create a minimal stub document for missing internal references.
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    ext = path.suffix.lower()

    if ext == ".md":
        title = path.stem.replace("_", " ")
        
        # Determine if this should have CAOS awareness based on path
        path_str = str(path).upper()
        include_caos = any(kw in path_str for kw in ["OPERATIONS", "NEURAL", "AI", "AUTOMATION", "SERVICES"])
        
        caos_section = f"\n{CAOS_LINE}" if include_caos else ""
        
        content = f"""# {title}

> Auto-generated placeholder document.
> Referenced from another document, but no content exists yet.

## Purpose

TBD – this document was created automatically because it was referenced but not present.

---

**Document Control:**
{AI_LINE}{caos_section}
- Version: 0.1
- Status: Draft – Auto-generated placeholder
- Last Updated: TBD
"""
        path.write_text(content, encoding="utf-8")
    elif ext == ".csv":
        content = "Field,Description\nTODO,Auto-generated placeholder CSV for referenced document\n"
        path.write_text(content, encoding="utf-8")
    elif ext == ".json":
        content = '{\n  "//": "Auto-generated placeholder JSON for referenced document"\n}\n'
        path.write_text(content, encoding="utf-8")
    else:
        # Shouldn't happen with current INTERNAL_EXTS, but guard anyway
        path.write_text("AUTO-GENERATED PLACEHOLDER\n", encoding="utf-8")


def process_file(path: pathlib.Path, fix: bool, doc_root: pathlib.Path, add_caos: bool = True) -> List[str]:
    """
    Process a single markdown file.
    Returns a list of issues (strings) if in check mode or if something was fixed.
    """
    text = path.read_text(encoding="utf-8")
    component_root = find_component_root(path, doc_root)

    issues: List[str] = []

    # 1) Document Control AI line (and CAOS if applicable)
    new_text, ai_changed = ensure_ai_line_in_doc_control(text, path, add_caos=add_caos)
    if ai_changed:
        msg = f"{path.relative_to(REPO_ROOT)}: missing AI generation line in Document Control"
        issues.append(msg)
        if fix:
            text = new_text

    # 2) Link internal paths
    linked_text, missing_paths, links_changed = link_internal_paths(text, component_root)
    if links_changed:
        msg = f"{path.relative_to(REPO_ROOT)}: added markdown links for internal document references"
        issues.append(msg)
        if fix:
            text = linked_text

    # 3) Create stubs for missing references
    for missing in missing_paths:
        rel = missing.relative_to(REPO_ROOT)
        if missing.suffix.lower() in INTERNAL_EXTS:
            issues.append(f"{path.relative_to(REPO_ROOT)}: referenced missing internal document {rel}")
            if fix:
                create_stub(missing)
                issues.append(f"  -> created stub {rel}")

    if fix and (ai_changed or links_changed):
        path.write_text(text, encoding="utf-8")

    return issues


def to_sarif(issues: List[str]) -> dict:
    """
    Convert issues list to SARIF format for GitHub Code Scanning integration.
    
    SARIF (Static Analysis Results Interchange Format) enables inline annotations
    in GitHub PRs, showing issues directly on the files/lines in the diff.
    """
    runs = [{
        "tool": {
            "driver": {
                "name": "doc_meta_enforcer",
                "version": "1.0",
                "informationUri": "https://github.com/AmedeoPelliccia/AMPEL360-BWB-H2-Hy-E/blob/main/tools/README.md#document-metadata-enforcer",
                "rules": [
                    {
                        "id": "missing-ai-attribution",
                        "name": "Missing AI Attribution",
                        "shortDescription": {"text": "Document Control section missing AI generation attribution"},
                        "helpUri": "https://github.com/AmedeoPelliccia/AMPEL360-BWB-H2-Hy-E/blob/main/tools/README.md#ai-attribution-management"
                    },
                    {
                        "id": "unlinked-internal-reference",
                        "name": "Unlinked Internal Reference",
                        "shortDescription": {"text": "Internal document reference not converted to hyperlink"},
                        "helpUri": "https://github.com/AmedeoPelliccia/AMPEL360-BWB-H2-Hy-E/blob/main/tools/README.md#internal-reference-linking"
                    },
                    {
                        "id": "missing-referenced-document",
                        "name": "Missing Referenced Document",
                        "shortDescription": {"text": "Referenced internal document does not exist"},
                        "helpUri": "https://github.com/AmedeoPelliccia/AMPEL360-BWB-H2-Hy-E/blob/main/tools/README.md#stub-generation"
                    }
                ]
            }
        },
        "results": []
    }]
    
    for msg in issues:
        # Parse issue message: "path: detail..."
        parts = str(msg).split(":", 1)
        if len(parts) < 2:
            continue
            
        file_path = parts[0].strip()
        detail = parts[1].strip()
        
        # Skip sub-messages (indented with "->")
        if file_path.startswith("->") or file_path.startswith(" "):
            continue
        
        # Determine rule ID based on message content
        rule_id = "missing-ai-attribution"
        if "markdown links" in detail or "internal document references" in detail:
            rule_id = "unlinked-internal-reference"
        elif "missing internal document" in detail:
            rule_id = "missing-referenced-document"
        
        runs[0]["results"].append({
            "ruleId": rule_id,
            "level": "warning",
            "message": {"text": detail},
            "locations": [{
                "physicalLocation": {
                    "artifactLocation": {"uri": file_path},
                    "region": {
                        "startLine": 1,
                        "startColumn": 1
                    }
                }
            }]
        })
    
    return {"version": "2.1.0", "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json", "runs": runs}


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Enforce document metadata standards across the repository"
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="Check only; do not modify files or create stubs (CI mode).",
    )
    parser.add_argument(
        "--fix",
        action="store_true",
        help="Apply fixes in-place and create stubs for missing documents.",
    )
    parser.add_argument(
        "--scope",
        choices=["ata02", "entire", "full"],
        default="ata02",
        help=(
            "Scope of documents to process. "
            "'ata02': ATA_02-OPERATIONS_INFORMATION only (default, CI-friendly). "
            "'entire' or 'full': Entire OPT-IN_FRAMEWORK directory (retrofit mode)."
        ),
    )
    parser.add_argument(
        "--sarif",
        type=pathlib.Path,
        help="Write SARIF report to this path for GitHub Code Scanning integration.",
    )
    args = parser.parse_args()

    if args.check and args.fix:
        print("Use either --check or --fix, not both.", file=sys.stderr)
        return 2

    fix = args.fix
    check = args.check or not args.fix  # default to check if no flag

    # Determine scope
    if args.scope in ["entire", "full"]:
        doc_root = ENTIRE_FRAMEWORK_ROOT
        print(f"[doc-meta] Processing entire OPT-IN_FRAMEWORK directory...")
        print(f"[doc-meta] Framework areas included:")
        for area, description in FRAMEWORK_AREAS.items():
            area_path = ENTIRE_FRAMEWORK_ROOT / area
            if area_path.exists():
                file_count = len(list(area_path.rglob("*.md")))
                print(f"  - {area}: {description} ({file_count} files)")
        add_caos = True  # Enable CAOS awareness for full repository processing
    else:
        doc_root = DEFAULT_DOC_ROOT
        print(f"[doc-meta] Processing ATA_02-OPERATIONS_INFORMATION directory...")
        add_caos = False  # Disable CAOS for focused ATA_02 processing

    all_issues: List[str] = []
    processed_count = 0

    for md in doc_root.rglob("*.md"):
        # Skip hidden directories and .git
        if any(part.startswith('.') for part in md.parts):
            continue
        issues = process_file(md, fix=fix, doc_root=doc_root, add_caos=add_caos)
        all_issues.extend(issues)
        processed_count += 1

    print(f"[doc-meta] Processed {processed_count} markdown files")
    
    # Generate SARIF report if requested (even when empty for workflow consistency)
    if args.sarif:
        args.sarif.parent.mkdir(parents=True, exist_ok=True)
        sarif_data = to_sarif(all_issues)
        args.sarif.write_text(json.dumps(sarif_data, indent=2), encoding="utf-8")
        print(f"[doc-meta] SARIF report written to {args.sarif}")

    if check:
        if all_issues:
            print("[doc-meta] Issues found:")
            for msg in all_issues:
                print("  -", msg)
            return 1
        else:
            print("[doc-meta] OK – all docs compliant.")
            return 0

    # fix mode
    if all_issues:
        print("[doc-meta] Applied fixes:")
        for msg in all_issues:
            print("  -", msg)
    else:
        print("[doc-meta] Nothing to fix.")
    return 0


if __name__ == "__main__":
    sys.exit(main())
